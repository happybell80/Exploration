{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1GTNi_-W3KohkHY7_r80n8XOdqRL7oP_I","authorship_tag":"ABX9TyOs26EXkqDaL6yA8/3Srf8r"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"premium"},"cells":[{"cell_type":"markdown","source":["# 6-7. 프로젝트: 멋진 작사가 만들기\n"],"metadata":{"id":"tn7OOzngwcYa"}},{"cell_type":"markdown","source":["- 라이브러리 버전을 확인해 봅니다\n"],"metadata":{"id":"zb4CVvI5wfid"}},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VHVt9ZJpvk3P","executionInfo":{"status":"ok","timestamp":1664952321067,"user_tz":-540,"elapsed":344,"user":{"displayName":"김종태","userId":"06122921812067136763"}},"outputId":"34f80765-606d-4607-ad44-0336eda30279"},"outputs":[{"output_type":"stream","name":"stdout","text":["2.8.2\n"]}],"source":["import glob  #glob 모듈의 glob 함수는 사용자가 제시한 조건에 맞는 파일명을 리스트 형식으로 반환한다\n","import os, re \n","import numpy as np\n","import tensorflow as tf\n","from sklearn.model_selection import train_test_split\n","\n","print(tf.__version__)"]},{"cell_type":"markdown","source":["### Step 1. 데이터 다운로드\n"],"metadata":{"id":"ZB2umyumwnBq"}},{"cell_type":"code","source":["txt_file_path = '/content/drive/MyDrive/Colab Notebooks/Exploration/lyrics/*'"],"metadata":{"id":"s2Uv67tqy_9Y","executionInfo":{"status":"ok","timestamp":1664950771746,"user_tz":-540,"elapsed":345,"user":{"displayName":"김종태","userId":"06122921812067136763"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["### Step 2. 데이터 읽어오기\n"],"metadata":{"id":"XzrRA19qwrzk"}},{"cell_type":"code","source":["# 데이터 읽어오기\n","\n","txt_list = glob.glob(txt_file_path) #txt_file_path 경로에 있는 모든 파일명을 리스트 형식으로 txt_list 에 할당\n","\n","raw_corpus = [] \n","\n","# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n","for txt_file in txt_list:\n","    with open(txt_file, \"r\") as f:\n","        raw = f.read().splitlines() #read() : 파일 전체의 내용을 하나의 문자열로 읽어온다. , splitlines()  : 여러라인으로 구분되어 있는 문자열을 한라인씩 분리하여 리스트로 반환\n","        raw_corpus.extend(raw) # extend() : 리스트함수로 추가적인 내용을 연장 한다.\n","\n","print(\"데이터 크기:\", len(raw_corpus))\n","print(\"Examples:\\n\", raw_corpus[:3])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w5AOlsnzvr6E","executionInfo":{"status":"ok","timestamp":1664950788373,"user_tz":-540,"elapsed":665,"user":{"displayName":"김종태","userId":"06122921812067136763"}},"outputId":"e5e33d2c-8aa1-4920-967f-45f8b2245cad"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["데이터 크기: 187088\n","Examples:\n"," ['Looking for some education', 'Made my way into the night', 'All that bullshit conversation']\n"]}]},{"cell_type":"markdown","source":["### Step 3. 데이터 정제\n"],"metadata":{"id":"vT4f-Akcx6AS"}},{"cell_type":"code","source":["# 입력된 문장을\n","#     1. 소문자로 바꾸고, 양쪽 공백을 지웁니다\n","#     2. 특수문자 양쪽에 공백을 넣고\n","#     3. 여러개의 공백은 하나의 공백으로 바꿉니다\n","#     4. a-zA-Z?.!,'¿가 아닌 모든 문자를 하나의 공백으로 바꿉니다 (' 추가)\n","#     5. 다시 양쪽 공백을 지웁니다\n","#     6. 문장 시작에는 <start>, 끝에는 <end>를 추가합니다\n","# 이 순서로 처리해주면 문제가 되는 상황을 방지할 수 있겠네요!\n","def preprocess_sentence(sentence):\n","    sentence = sentence.lower().strip() # 1\n","    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 2\n","    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 3\n","    sentence = re.sub(r\"[^a-zA-Z?.!,'¿]+\", \" \", sentence) # 4\n","    sentence = sentence.strip() # 5\n","    sentence = '<start> ' + sentence + ' <end>' # 6\n","    return sentence\n","\n","# 이 문장이 어떻게 필터링되는지 확인해 보세요.\n","print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"biRXMZpLzd5U","executionInfo":{"status":"ok","timestamp":1664951121380,"user_tz":-540,"elapsed":372,"user":{"displayName":"김종태","userId":"06122921812067136763"}},"outputId":"38d6d388-b4fc-400a-9c76-edbb98923399"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["<start> this is sample sentence . <end>\n"]}]},{"cell_type":"code","source":["# 여기에 정제된 문장을 모을겁니다\n","corpus = []\n","\n","'''이번에는 문장을 토큰화 했을 때 토큰의 개수가 15개를 넘어가는 문장을 학습 데이터에서 제외하기 를 권합니다.'''\n","\n","# raw_corpus list에 저장된 문장들을 순서대로 반환하여 sentence에 저장\n","for sentence in raw_corpus:\n","    # 우리가 원하지 않는 문장은 건너뜁니다\n","    if len(sentence) == 0: continue\n","    if sentence[-1] == \":\": continue\n","    if len(sentence.split(' '))>15 : continue \n","    \n","    # 앞서 구현한 preprocess_sentence() 함수를 이용하여 문장을 정제를 하고 담아주세요\n","    preprocessed_sentence = preprocess_sentence(sentence)\n","    corpus.append(preprocessed_sentence)\n","        \n","# 정제된 결과를 10개만 확인해보죠\n","corpus[:10]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sAwqwDLXzjz8","executionInfo":{"status":"ok","timestamp":1664955699652,"user_tz":-540,"elapsed":1793,"user":{"displayName":"김종태","userId":"06122921812067136763"}},"outputId":"b1f49371-9563-4999-f880-3ec733a3f9b0"},"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['<start> looking for some education <end>',\n"," '<start> made my way into the night <end>',\n"," '<start> all that bullshit conversation <end>',\n"," \"<start> baby , can't you read the signs ? i won't bore you with the details , baby <end>\",\n"," \"<start> i don't even wanna waste your time <end>\",\n"," \"<start> let's just say that maybe <end>\",\n"," '<start> you could help me ease my mind <end>',\n"," \"<start> i ain't mr . right but if you're looking for fast love <end>\",\n"," \"<start> if that's love in your eyes <end>\",\n"," \"<start> it's more than enough <end>\"]"]},"metadata":{},"execution_count":21}]},{"cell_type":"markdown","source":["### Step 4. 평가 데이터셋 분리\n"],"metadata":{"id":"xqRBEMfxyEhA"}},{"cell_type":"code","source":["'''훈련 데이터와 평가 데이터를 분리하세요!\n","\n","tokenize() 함수로 데이터를 Tensor로 변환한 후, \n","sklearn 모듈의 train_test_split() 함수를 사용해 훈련 데이터와 평가 데이터를 분리하도록 하겠습니다. \n","단어장의 크기는 12,000 이상 으로 설정하세요! \n","총 데이터의 20% 를 평가 데이터셋으로 사용해 주세요!'''"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":54},"id":"MQiL6ZewyDdQ","executionInfo":{"status":"ok","timestamp":1664955702814,"user_tz":-540,"elapsed":344,"user":{"displayName":"김종태","userId":"06122921812067136763"}},"outputId":"f3b8a9a0-d3b0-4805-9c9b-fc405f8e74d3"},"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'훈련 데이터와 평가 데이터를 분리하세요!\\n\\ntokenize() 함수로 데이터를 Tensor로 변환한 후, \\nsklearn 모듈의 train_test_split() 함수를 사용해 훈련 데이터와 평가 데이터를 분리하도록 하겠습니다. \\n단어장의 크기는 12,000 이상 으로 설정하세요! \\n총 데이터의 20% 를 평가 데이터셋으로 사용해 주세요!'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":22}]},{"cell_type":"markdown","source":["- 토큰화"],"metadata":{"id":"_39qkkdM4EfG"}},{"cell_type":"code","source":["# 토큰화 할 때 텐서플로우의 Tokenizer와 pad_sequences를 사용합니다\n","# 더 잘 알기 위해 아래 문서들을 참고하면 좋습니다\n","# https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer\n","# https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences\n","def tokenize(corpus):\n","    # 12000단어를 기억할 수 있는 tokenizer를 만들겁니다\n","    # 우리는 이미 문장을 정제했으니 filters가 필요없어요\n","    # 12000단어에 포함되지 못한 단어는 '<unk>'로 바꿀거에요\n","    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n","        num_words=12000,    \n","        filters=' ',\n","        oov_token=\"<unk>\"\n","    )\n","    # corpus를 이용해 tokenizer 내부의 단어장을 완성합니다\n","    # tokenizer.fit_on_texts(texts): 문자 데이터를 입력받아 리스트의 형태로 변환하는 메서드\n","    tokenizer.fit_on_texts(corpus)\n","    # 준비한 tokenizer를 이용해 corpus를 Tensor로 변환합니다\n","    # tokenizer.texts_to_sequences(texts): 텍스트 안의 단어들을 숫자의 시퀀스 형태로 변환하는 메서드\n","    tensor = tokenizer.texts_to_sequences(corpus)   \n","    # 입력 데이터의 시퀀스 길이를 일정하게 맞춰줍니다\n","    # 만약 시퀀스가 짧다면 문장 뒤에 패딩을 붙여 길이를 맞춰줍니다.\n","    # 문장 앞에 패딩을 붙여 길이를 맞추고 싶다면 padding='pre'를 사용합니다\n","    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  \n","    \n","    print(tensor,tokenizer)\n","    return tensor, tokenizer\n","\n","tensor, tokenizer = tokenize(corpus)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1-2yRZZ74DgJ","executionInfo":{"status":"ok","timestamp":1664955737204,"user_tz":-540,"elapsed":3802,"user":{"displayName":"김종태","userId":"06122921812067136763"}},"outputId":"a96f3541-2ccd-4181-c6e6-751f4f8d63d5"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["[[  2 299  26 ...   0   0   0]\n"," [  2 226  12 ...   0   0   0]\n"," [  2  22  15 ...   0   0   0]\n"," ...\n"," [  2  29  19 ...   0   0   0]\n"," [  2 344  19 ...   0   0   0]\n"," [  2  38 131 ...   0   0   0]] <keras_preprocessing.text.Tokenizer object at 0x7f4ba425ec50>\n"]}]},{"cell_type":"code","source":["# tokenizer.index_word: 현재 계산된 단어의 인덱스와 인덱스에 해당하는 단어를 dictionary 형대로 반환 (Ex. {index: '~~', index: '~~', ...})\n","for idx in tokenizer.index_word:\n","    print(idx, \":\", tokenizer.index_word[idx])\n","\n","    if idx >= 5: break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ge-uN_lC4QHX","executionInfo":{"status":"ok","timestamp":1664955739756,"user_tz":-540,"elapsed":969,"user":{"displayName":"김종태","userId":"06122921812067136763"}},"outputId":"acfb8aa1-b898-4dd1-b289-d716446dfd23"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["1 : <unk>\n","2 : <start>\n","3 : <end>\n","4 : ,\n","5 : the\n"]}]},{"cell_type":"code","source":["# tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성합니다\n","# 마지막 토큰은 <end>가 아니라 <pad>일 가능성이 높습니다.\n","src_input = tensor[:, :-1]  \n","# tensor에서 <start>를 잘라내서 타겟 문장을 생성합니다.\n","tgt_input = tensor[:, 1:]    \n","\n","print(src_input[0])\n","print(tgt_input[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JTCxh6qX6uX3","executionInfo":{"status":"ok","timestamp":1664955741913,"user_tz":-540,"elapsed":3,"user":{"displayName":"김종태","userId":"06122921812067136763"}},"outputId":"6ca3bd8f-a3ff-41b6-9522-79845b567fd8"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["[   2  299   26   93 4679    3    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0]\n","[ 299   26   93 4679    3    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0]\n"]}]},{"cell_type":"code","source":["# 훈련 데이터와 평가 데이터를 분리\n","\n","enc_train, enc_val, dec_train, dec_val = train_test_split(src_input,\n","                                                          tgt_input,\n","                                                          test_size=0.2,\n","                                                          random_state=15)"],"metadata":{"id":"nsfl9eI343CV","executionInfo":{"status":"ok","timestamp":1664955744776,"user_tz":-540,"elapsed":336,"user":{"displayName":"김종태","userId":"06122921812067136763"}}},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":["### Step 5. 인공지능 만들기\n"],"metadata":{"id":"GVq1PKHgyJjf"}},{"cell_type":"code","source":["BUFFER_SIZE = len(enc_train)\n","BATCH_SIZE = 256\n","steps_per_epoch = len(enc_train) // BATCH_SIZE\n","\n"," # tokenizer가 구축한 단어사전 내 12000개와, 여기 포함되지 않은 0:<pad>를 포함하여 12001개\n"," # tokenizer.num_words: 주어진 데이터의 문장들에서 빈도수가 높은 n개의 단어만 선택\n"," # tokenize() 함수에서 num_words를 12000개로 선언했기 때문에, tokenizer.num_words의 값은 12000\n","VOCAB_SIZE = tokenizer.num_words + 1   \n","\n","# 준비한 데이터 소스로부터 데이터셋을 만듭니다\n","# 데이터셋에 대해서는 아래 문서를 참고하세요\n","# 자세히 알아둘수록 도움이 많이 되는 중요한 문서입니다\n","# https://www.tensorflow.org/api_docs/python/tf/data/Dataset\n","dataset = tf.data.Dataset.from_tensor_slices((enc_train, dec_train))\n","dataset = dataset.shuffle(BUFFER_SIZE)\n","dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n","dataset"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OJNP2IzU7BPy","executionInfo":{"status":"ok","timestamp":1664955751130,"user_tz":-540,"elapsed":2868,"user":{"displayName":"김종태","userId":"06122921812067136763"}},"outputId":"5bba9060-72ae-487d-f75f-0ef4936f0b76"},"execution_count":28,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<BatchDataset element_spec=(TensorSpec(shape=(256, 32), dtype=tf.int32, name=None), TensorSpec(shape=(256, 32), dtype=tf.int32, name=None))>"]},"metadata":{},"execution_count":28}]},{"cell_type":"code","source":["class TextGenerator(tf.keras.Model):\n","    def __init__(self, vocab_size, embedding_size, hidden_size):\n","        super().__init__()\n","        # Embedding 레이어, 2개의 LSTM 레이어, 1개의 Dense 레이어로 구성되어 있다.\n","        # Embedding 레이어는 단어 사전의 인덱스 값을 해당 인덱스 번째의 워드 벡터로 바꿔준다.\n","        # 이 워드 벡터는 의미 벡터 공간에서 단어의 추상적 표현으로 사용된다. \n","        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size) \n","        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)  \n","        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n","        self.linear = tf.keras.layers.Dense(vocab_size)\n","        \n","    def call(self, x):\n","        out = self.embedding(x)\n","        out = self.rnn_1(out)\n","        out = self.rnn_2(out)\n","        out = self.linear(out)\n","        \n","        return out\n","# embedding size 값이 커질수록 단어의 추상적인 특징들을 더 잡아낼 수 있지만\n","# 그만큼 충분한 데이터가 없으면 안좋은 결과 값을 가져옵니다!   \n","embedding_size = 256          # 워드 벡터의 차원수를 말하며 단어가 추상적으로 표현되는 크기입니다.\n","hidden_size = 1024            # 모델에 얼마나 많은 일꾼을 둘 것인가? 정도로 이해하면 좋다.\n","model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size) \n","# tokenizer.num_words에 +1인 이유는 문장에 없는 pad가 사용되었기 때문이다."],"metadata":{"id":"LzwOQJku7CoF","executionInfo":{"status":"ok","timestamp":1664955753171,"user_tz":-540,"elapsed":6,"user":{"displayName":"김종태","userId":"06122921812067136763"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["# 데이터셋에서 데이터 한 배치만 불러오는 방법입니다.\n","# 지금은 동작 원리에 너무 빠져들지 마세요~\n","for src_sample, tgt_sample in dataset.take(1): break\n","\n","# 한 배치만 불러온 데이터를 모델에 넣어봅니다\n","model(src_sample)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vKqcTndP7FQY","executionInfo":{"status":"ok","timestamp":1664955761678,"user_tz":-540,"elapsed":6545,"user":{"displayName":"김종태","userId":"06122921812067136763"}},"outputId":"f7524182-4d04-47ad-b143-d3fdfbf73ed0"},"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(256, 32, 12001), dtype=float32, numpy=\n","array([[[ 5.27154270e-06,  2.16574059e-04, -1.25077597e-04, ...,\n","         -1.54405861e-04, -1.56952286e-04,  3.93029513e-05],\n","        [ 4.41452459e-04,  3.87898413e-04, -8.90837182e-05, ...,\n","         -1.55509606e-05, -1.73156790e-04,  1.67273192e-05],\n","        [ 8.05387157e-04,  5.73716185e-04,  7.40415562e-05, ...,\n","          1.98860565e-04, -1.31061155e-04,  1.77564230e-04],\n","        ...,\n","        [-1.42546860e-03,  5.17456734e-04, -2.30724434e-03, ...,\n","          4.13584930e-04, -1.98521931e-03, -5.59942424e-03],\n","        [-1.43798545e-03,  5.03595453e-04, -2.29601120e-03, ...,\n","          4.28470317e-04, -1.98510126e-03, -5.65975672e-03],\n","        [-1.44684641e-03,  4.90882318e-04, -2.28528236e-03, ...,\n","          4.40307456e-04, -1.98442023e-03, -5.71117178e-03]],\n","\n","       [[ 5.27154270e-06,  2.16574059e-04, -1.25077597e-04, ...,\n","         -1.54405861e-04, -1.56952286e-04,  3.93029513e-05],\n","        [-1.01548125e-04,  3.48319154e-04, -4.24422615e-05, ...,\n","         -5.50890400e-04, -5.87600698e-05,  1.37445077e-05],\n","        [-4.63501347e-05,  2.48278579e-04, -9.74751529e-05, ...,\n","         -1.01559469e-03,  2.74040940e-04,  2.48573160e-05],\n","        ...,\n","        [-1.45865348e-03,  4.61822725e-04, -2.24939059e-03, ...,\n","          4.33517009e-04, -1.96393067e-03, -5.74727450e-03],\n","        [-1.46066665e-03,  4.53818066e-04, -2.24539265e-03, ...,\n","          4.42353630e-04, -1.96441845e-03, -5.78341819e-03],\n","        [-1.46180508e-03,  4.46204300e-04, -2.24130065e-03, ...,\n","          4.49648884e-04, -1.96498353e-03, -5.81401959e-03]],\n","\n","       [[ 5.27154270e-06,  2.16574059e-04, -1.25077597e-04, ...,\n","         -1.54405861e-04, -1.56952286e-04,  3.93029513e-05],\n","        [ 2.01959381e-04,  3.84656189e-04, -3.64500709e-04, ...,\n","         -2.50664278e-04, -1.90914157e-04,  1.22930287e-04],\n","        [ 3.40960571e-04,  6.40902377e-04, -4.71487292e-04, ...,\n","         -5.54113009e-04,  7.57943344e-05,  2.52965634e-04],\n","        ...,\n","        [-1.28781446e-03,  6.48350397e-04, -2.43536592e-03, ...,\n","          3.17505008e-04, -1.87435828e-03, -5.55267185e-03],\n","        [-1.31441583e-03,  6.24404987e-04, -2.40774686e-03, ...,\n","          3.42950108e-04, -1.88498187e-03, -5.62109519e-03],\n","        [-1.33610668e-03,  6.01389853e-04, -2.38227705e-03, ...,\n","          3.64465290e-04, -1.89418602e-03, -5.67904115e-03]],\n","\n","       ...,\n","\n","       [[ 5.27154270e-06,  2.16574059e-04, -1.25077597e-04, ...,\n","         -1.54405861e-04, -1.56952286e-04,  3.93029513e-05],\n","        [-6.11793294e-05,  3.55536031e-04, -1.79301976e-04, ...,\n","         -2.85233138e-04, -3.02667846e-04,  2.25175929e-04],\n","        [-1.96757610e-04,  3.09798750e-04, -3.08075774e-04, ...,\n","         -2.80486245e-04, -2.44183058e-04,  5.44577953e-04],\n","        ...,\n","        [-1.48983207e-03,  4.76569636e-04, -2.30478356e-03, ...,\n","          4.58285154e-04, -1.99209829e-03, -5.76164480e-03],\n","        [-1.48611050e-03,  4.67400940e-04, -2.29097344e-03, ...,\n","          4.59734001e-04, -1.98743003e-03, -5.79559244e-03],\n","        [-1.48258137e-03,  4.58587019e-04, -2.27880129e-03, ...,\n","          4.61296964e-04, -1.98358158e-03, -5.82425576e-03]],\n","\n","       [[ 5.27154270e-06,  2.16574059e-04, -1.25077597e-04, ...,\n","         -1.54405861e-04, -1.56952286e-04,  3.93029513e-05],\n","        [-1.05676743e-04,  3.25514295e-04, -1.26436687e-04, ...,\n","          9.30120950e-05, -4.62078227e-04, -1.25267979e-04],\n","        [-9.02192005e-06,  3.12760472e-04, -7.73583888e-05, ...,\n","          1.37577345e-05, -4.98524925e-04, -2.66391697e-04],\n","        ...,\n","        [-1.43679278e-03,  5.84149093e-04, -2.24194233e-03, ...,\n","          2.17091510e-04, -1.93282007e-03, -5.06280875e-03],\n","        [-1.46192231e-03,  5.85621223e-04, -2.24441150e-03, ...,\n","          2.64406204e-04, -1.94454147e-03, -5.20035392e-03],\n","        [-1.47744035e-03,  5.82396402e-04, -2.24529672e-03, ...,\n","          3.03067936e-04, -1.95184536e-03, -5.31837763e-03]],\n","\n","       [[ 5.27154270e-06,  2.16574059e-04, -1.25077597e-04, ...,\n","         -1.54405861e-04, -1.56952286e-04,  3.93029513e-05],\n","        [ 6.63093597e-05,  2.95244012e-04, -2.58046377e-04, ...,\n","         -4.05945408e-04, -1.25970124e-04,  3.55330296e-04],\n","        [ 1.30796368e-04,  2.56880594e-04, -5.05786331e-04, ...,\n","         -5.37902815e-04, -2.45779549e-04,  6.33702613e-04],\n","        ...,\n","        [-1.49134162e-03,  5.37085696e-04, -2.35472806e-03, ...,\n","          3.17283033e-04, -1.98712712e-03, -5.36385784e-03],\n","        [-1.50877249e-03,  5.25309297e-04, -2.33751908e-03, ...,\n","          3.41421372e-04, -1.98783702e-03, -5.45318704e-03],\n","        [-1.51913404e-03,  5.14128653e-04, -2.32157251e-03, ...,\n","          3.62198945e-04, -1.98656740e-03, -5.52973943e-03]]],\n","      dtype=float32)>"]},"metadata":{},"execution_count":30}]},{"cell_type":"code","source":["# 모델의 구조를 확인합니다.\n","model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PSde355e7IbW","executionInfo":{"status":"ok","timestamp":1664955763890,"user_tz":-540,"elapsed":18,"user":{"displayName":"김종태","userId":"06122921812067136763"}},"outputId":"444bcb22-dc8b-47a1-d805-b9c97ca24ba6"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"text_generator\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       multiple                  3072256   \n","                                                                 \n"," lstm (LSTM)                 multiple                  5246976   \n","                                                                 \n"," lstm_1 (LSTM)               multiple                  8392704   \n","                                                                 \n"," dense (Dense)               multiple                  12301025  \n","                                                                 \n","=================================================================\n","Total params: 29,012,961\n","Trainable params: 29,012,961\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["# optimizer와 loss등은 차차 배웁니다\n","# 혹시 미리 알고 싶다면 아래 문서를 참고하세요\n","# https://www.tensorflow.org/api_docs/python/tf/keras/optimizers\n","# https://www.tensorflow.org/api_docs/python/tf/keras/losses\n","# 양이 상당히 많은 편이니 지금 보는 것은 추천하지 않습니다\n","\n","# Adam 알고리즘을 구현하는 optimzier이며 어떤 optimzier를 써야할지 모른다면 Adam을 쓰는 것도 방법이다.\n","# 우리가 학습을 할 때 최대한 틀리지 않는 방향으로 학습을 해야한다.\n","# 여기서 얼마나 틀리는지(loss)를 알게하는 함수가 손실함수 이다.\n","# 이 손실함수의 최소값을 찾는 것을 학습의 목표로 하며 여기서 최소값을 찾아가는 과정을 optimization 이라하고\n","# 이를 수행하는 알고리즘을 optimizer(최적화)라고 한다.\n","\n","optimizer = tf.keras.optimizers.Adam() # Adam은 현재 가장 많이 사용하는 옵티마이저이다. 자세한 내용은 차차 배운다.\n","loss = tf.keras.losses.SparseCategoricalCrossentropy( # 훈련 데이터의 라벨이 정수의 형태로 제공될 때 사용하는 손실함수이다.\n","    from_logits=True, # 기본값은 False이다. 모델에 의해 생성된 출력 값이 정규화되지 않았음을 손실 함수에 알려준다. 즉 softmax함수가 적용되지 않았다는걸 의미한다. \n","    reduction='none'  # 기본값은 SUM이다. 각자 나오는 값의 반환 원할 때 None을 사용한다.\n",")\n","# 모델을 학습시키키 위한 학습과정을 설정하는 단계이다.\n","model.compile(loss=loss, optimizer=optimizer) # 손실함수와 훈련과정을 설정했다."],"metadata":{"id":"TLO42dnl7LVT","executionInfo":{"status":"ok","timestamp":1664955766113,"user_tz":-540,"elapsed":348,"user":{"displayName":"김종태","userId":"06122921812067136763"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["'''\n","모델의 Embedding Size와 Hidden Size를 조절하며 10 Epoch 안에 val_loss 값을 2.2 수준으로 줄일 수 있는 모델을 설계하세요!\n","\n","잘 설계한 모델을 학습하려면, model.fit() 함수를 사용해야 합니다. model.fit() 함수에는 다양한 인자를 넣어주어야 하는데, \n","가장 기본적인 인자로는 데이터셋과 epochs가 있습니다. '5. 실습 (2) 인공지능 학습시키기'에서의 예시와 같이 말이죠.\n","'''\n","\n","model.fit(dataset, epochs=30)\n","\n","'''하지만 model.fit() 함수의 epochs를 아무리 크게 넣는다 해도 val_loss 값은 2.2 아래로 떨어지지 않습니다. \n","이럴 경우는 batch size를 변경하는 것과 같이 model.fit() 함수에 다양한 인자를 넣어주면 해결될 수도 있습니다. \n","자세한 내용은 https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit 를 참고하세요!'''\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"pSqWJoKRyDfv","executionInfo":{"status":"ok","timestamp":1664958946367,"user_tz":-540,"elapsed":3173175,"user":{"displayName":"김종태","userId":"06122921812067136763"}},"outputId":"b2790917-b1c7-4f04-cacd-dfa0c0447055"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/30\n","525/525 [==============================] - 104s 193ms/step - loss: 1.6834\n","Epoch 2/30\n","525/525 [==============================] - 101s 193ms/step - loss: 1.4430\n","Epoch 3/30\n","525/525 [==============================] - 101s 193ms/step - loss: 1.3595\n","Epoch 4/30\n","525/525 [==============================] - 101s 193ms/step - loss: 1.3051\n","Epoch 5/30\n","525/525 [==============================] - 101s 193ms/step - loss: 1.2582\n","Epoch 6/30\n","525/525 [==============================] - 101s 193ms/step - loss: 1.2150\n","Epoch 7/30\n","525/525 [==============================] - 101s 193ms/step - loss: 1.1741\n","Epoch 8/30\n","525/525 [==============================] - 102s 193ms/step - loss: 1.1349\n","Epoch 9/30\n","525/525 [==============================] - 102s 193ms/step - loss: 1.0975\n","Epoch 10/30\n","525/525 [==============================] - 102s 193ms/step - loss: 1.0620\n","Epoch 11/30\n","525/525 [==============================] - 102s 193ms/step - loss: 1.0277\n","Epoch 12/30\n","525/525 [==============================] - 101s 193ms/step - loss: 0.9947\n","Epoch 13/30\n","525/525 [==============================] - 101s 193ms/step - loss: 0.9631\n","Epoch 14/30\n","525/525 [==============================] - 102s 193ms/step - loss: 0.9327\n","Epoch 15/30\n","525/525 [==============================] - 101s 193ms/step - loss: 0.9037\n","Epoch 16/30\n","525/525 [==============================] - 102s 193ms/step - loss: 0.8757\n","Epoch 17/30\n","525/525 [==============================] - 102s 193ms/step - loss: 0.8488\n","Epoch 18/30\n","525/525 [==============================] - 102s 193ms/step - loss: 0.8229\n","Epoch 19/30\n","525/525 [==============================] - 102s 194ms/step - loss: 0.7977\n","Epoch 20/30\n","525/525 [==============================] - 102s 194ms/step - loss: 0.7735\n","Epoch 21/30\n","525/525 [==============================] - 102s 194ms/step - loss: 0.7500\n","Epoch 22/30\n","525/525 [==============================] - 102s 194ms/step - loss: 0.7270\n","Epoch 23/30\n","525/525 [==============================] - 102s 194ms/step - loss: 0.7047\n","Epoch 24/30\n","525/525 [==============================] - 102s 194ms/step - loss: 0.6833\n","Epoch 25/30\n","525/525 [==============================] - 102s 194ms/step - loss: 0.6627\n","Epoch 26/30\n","525/525 [==============================] - 102s 194ms/step - loss: 0.6426\n","Epoch 27/30\n","525/525 [==============================] - 102s 194ms/step - loss: 0.6233\n","Epoch 28/30\n","525/525 [==============================] - 102s 194ms/step - loss: 0.6049\n","Epoch 29/30\n","525/525 [==============================] - 102s 194ms/step - loss: 0.5874\n","Epoch 30/30\n","525/525 [==============================] - 102s 194ms/step - loss: 0.5709\n"]},{"output_type":"execute_result","data":{"text/plain":["'하지만 model.fit() 함수의 epochs를 아무리 크게 넣는다 해도 val_loss 값은 2.2 아래로 떨어지지 않습니다. \\n이럴 경우는 batch size를 변경하는 것과 같이 model.fit() 함수에 다양한 인자를 넣어주면 해결될 수도 있습니다. \\n자세한 내용은 https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit 를 참고하세요!'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":33}]},{"cell_type":"markdown","source":["### Step 6. 모델 평가"],"metadata":{"id":"ITY7-DIYSW-L"}},{"cell_type":"code","source":["test_loss, test_accuracy = model.evaluate(enc_val, dec_val, verbose=2)\n","print(f\"test_loss: {test_loss}\")\n","print(f\"test_accuracy: {test_accuracy}\")\n","\n","## 당연히 안되겠지요... "],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":227},"id":"QhwSoLg0SbRM","executionInfo":{"status":"error","timestamp":1664959069342,"user_tz":-540,"elapsed":15924,"user":{"displayName":"김종태","userId":"06122921812067136763"}},"outputId":"a0063bd9-ab59-4b48-acca-916a772229af"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["1052/1052 - 15s - loss: 1.1527 - 15s/epoch - 14ms/step\n"]},{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-34-add993d05cd2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"test_loss: {test_loss}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"test_accuracy: {test_accuracy}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable float object"]}]},{"cell_type":"markdown","source":["### 마지막으로 멋진 모델이 생성한 가사 한 줄을 제출하시길 바랍니다!\n","\n"],"metadata":{"id":"IKBe_eK-ympv"}},{"cell_type":"code","source":["#문장생성 함수 정의\n","#모델에게 시작 문장을 전달하면 모델이 시작 문장을 바탕으로 작문을 진행\n","def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20): #시작 문자열을 init_sentence 로 받으며 디폴트값은 <start> 를 받는다\n","    # 테스트를 위해서 입력받은 init_sentence도 텐서로 변환합니다\n","    test_input = tokenizer.texts_to_sequences([init_sentence]) #텍스트 안의 단어들을 숫자의 시퀀스의 형태로 변환\n","    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n","    end_token = tokenizer.word_index[\"<end>\"]\n","\n","    # 단어 하나씩 예측해 문장을 만듭니다\n","    #    1. 입력받은 문장의 텐서를 입력합니다\n","    #    2. 예측된 값 중 가장 높은 확률인 word index를 뽑아냅니다\n","    #    3. 2에서 예측된 word index를 문장 뒤에 붙입니다\n","    #    4. 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마칩니다 (도달 하지 못하였으면 while 루프를 돌면서 다음 단어를 예측)\n","    while True: #루프를 돌면서 init_sentence에 단어를 하나씩 생성성\n","        # 1\n","        predict = model(test_tensor) \n","        # 2\n","        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n","        # 3 \n","        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n","        # 4 \n","        if predict_word.numpy()[0] == end_token: break\n","        if test_tensor.shape[1] >= max_len: break\n","\n","    generated = \"\"\n","    # tokenizer를 이용해 word index를 단어로 하나씩 변환합니다 \n","    for word_index in test_tensor[0].numpy():\n","        generated += tokenizer.index_word[word_index] + \" \"\n","\n","    return generated #최종적으로 모델이 생성한 문장을 반환"],"metadata":{"id":"BjyUs0DH7R3A","executionInfo":{"status":"ok","timestamp":1664959358449,"user_tz":-540,"elapsed":605,"user":{"displayName":"김종태","userId":"06122921812067136763"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["lyricist = model\n","generate_text(lyricist, tokenizer, init_sentence=\"<start> i love\", max_len=20)\n","# generate_text 함수에 lyricist 라 정의한 모델을 이용해서 ilove 로 시작되는 문장을 생성"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"vo8hE7NdyDl7","executionInfo":{"status":"ok","timestamp":1664959369713,"user_tz":-540,"elapsed":10,"user":{"displayName":"김종태","userId":"06122921812067136763"}},"outputId":"39e7d4ae-d086-4464-9730-bb47189e304c"},"execution_count":37,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'<start> i love you so much , i love you so much , i love you so much <end> '"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":37}]},{"cell_type":"code","source":[],"metadata":{"id":"ALSsv1d_yDos"},"execution_count":null,"outputs":[]}]}