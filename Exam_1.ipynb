{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c82d332e",
   "metadata": {},
   "source": [
    "# 1-11. 프로젝트 (1) load_digits : 손글씨를 분류해 봅시다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "50f4ee8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "[[ 0.  0.  5. ...  0.  0.  0.]\n",
      " [ 0.  0.  0. ... 10.  0.  0.]\n",
      " [ 0.  0.  0. ... 16.  9.  0.]\n",
      " ...\n",
      " [ 0.  0.  1. ...  0.  0.  0.]\n",
      " [ 0.  0. 10. ...  0.  0.  0.]\n",
      " [ 0.  0.  6. ... 13. 11.  1.]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.90      0.95        31\n",
      "           1       0.84      0.82      0.83        38\n",
      "           2       0.73      0.87      0.80        38\n",
      "           3       0.71      0.74      0.73        27\n",
      "           4       0.94      0.80      0.87        41\n",
      "           5       0.87      0.94      0.90        35\n",
      "           6       0.87      0.89      0.88        38\n",
      "           7       0.91      0.94      0.93        34\n",
      "           8       0.79      0.77      0.78        35\n",
      "           9       0.80      0.77      0.79        43\n",
      "\n",
      "    accuracy                           0.84       360\n",
      "   macro avg       0.85      0.85      0.84       360\n",
      "weighted avg       0.85      0.84      0.85       360\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.94      0.95        31\n",
      "           1       0.95      0.97      0.96        38\n",
      "           2       1.00      1.00      1.00        38\n",
      "           3       1.00      0.96      0.98        27\n",
      "           4       0.95      1.00      0.98        41\n",
      "           5       0.97      1.00      0.99        35\n",
      "           6       1.00      0.95      0.97        38\n",
      "           7       1.00      1.00      1.00        34\n",
      "           8       0.94      0.97      0.96        35\n",
      "           9       1.00      0.98      0.99        43\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.98        31\n",
      "           1       0.95      1.00      0.97        38\n",
      "           2       1.00      1.00      1.00        38\n",
      "           3       0.96      0.96      0.96        27\n",
      "           4       0.98      0.98      0.98        41\n",
      "           5       1.00      1.00      1.00        35\n",
      "           6       1.00      1.00      1.00        38\n",
      "           7       1.00      1.00      1.00        34\n",
      "           8       0.97      0.94      0.96        35\n",
      "           9       0.98      0.98      0.98        43\n",
      "\n",
      "    accuracy                           0.98       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.98      0.98       360\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97        31\n",
      "           1       1.00      0.87      0.93        38\n",
      "           2       1.00      0.97      0.99        38\n",
      "           3       0.96      0.85      0.90        27\n",
      "           4       1.00      0.95      0.97        41\n",
      "           5       0.90      1.00      0.95        35\n",
      "           6       1.00      0.97      0.99        38\n",
      "           7       0.94      1.00      0.97        34\n",
      "           8       0.72      0.97      0.83        35\n",
      "           9       0.97      0.84      0.90        43\n",
      "\n",
      "    accuracy                           0.94       360\n",
      "   macro avg       0.95      0.94      0.94       360\n",
      "weighted avg       0.95      0.94      0.94       360\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        31\n",
      "           1       0.95      0.97      0.96        38\n",
      "           2       1.00      0.97      0.99        38\n",
      "           3       0.96      0.93      0.94        27\n",
      "           4       0.98      1.00      0.99        41\n",
      "           5       0.97      0.97      0.97        35\n",
      "           6       1.00      0.97      0.99        38\n",
      "           7       1.00      1.00      1.00        34\n",
      "           8       0.92      0.97      0.94        35\n",
      "           9       0.98      0.95      0.96        43\n",
      "\n",
      "    accuracy                           0.97       360\n",
      "   macro avg       0.98      0.97      0.97       360\n",
      "weighted avg       0.98      0.97      0.98       360\n",
      "\n",
      "[[28  0  1  0  0  0  0  0  2  0]\n",
      " [ 0 31  2  2  1  0  1  0  1  0]\n",
      " [ 0  0 33  1  0  1  0  0  2  1]\n",
      " [ 0  1  1 20  0  1  1  0  1  2]\n",
      " [ 0  0  0  1 33  0  3  2  0  2]\n",
      " [ 0  0  1  0  0 33  0  1  0  0]\n",
      " [ 0  1  2  0  1  0 34  0  0  0]\n",
      " [ 0  0  0  1  0  0  0 32  0  1]\n",
      " [ 0  1  3  1  0  1  0  0 27  2]\n",
      " [ 0  3  2  2  0  2  0  0  1 33]]\n",
      "\n",
      "[[29  0  0  0  2  0  0  0  0  0]\n",
      " [ 0 37  0  0  0  1  0  0  0  0]\n",
      " [ 0  0 38  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 26  0  0  0  0  1  0]\n",
      " [ 0  0  0  0 41  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 35  0  0  0  0]\n",
      " [ 1  1  0  0  0  0 36  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 34  0  0]\n",
      " [ 0  1  0  0  0  0  0  0 34  0]\n",
      " [ 0  0  0  0  0  0  0  0  1 42]]\n",
      "\n",
      "[[30  0  0  0  1  0  0  0  0  0]\n",
      " [ 0 38  0  0  0  0  0  0  0  0]\n",
      " [ 0  0 38  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 26  0  0  0  0  1  0]\n",
      " [ 0  0  0  0 40  0  0  0  0  1]\n",
      " [ 0  0  0  0  0 35  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 38  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 34  0  0]\n",
      " [ 0  2  0  0  0  0  0  0 33  0]\n",
      " [ 0  0  0  1  0  0  0  0  0 42]]\n",
      "\n",
      "[[30  0  0  0  0  0  0  0  1  0]\n",
      " [ 0 33  0  0  0  1  0  0  4  0]\n",
      " [ 0  0 37  0  0  0  0  0  1  0]\n",
      " [ 0  0  0 23  0  1  0  0  2  1]\n",
      " [ 1  0  0  0 39  0  0  0  1  0]\n",
      " [ 0  0  0  0  0 35  0  0  0  0]\n",
      " [ 0  0  0  0  0  1 37  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 34  0  0]\n",
      " [ 0  0  0  0  0  1  0  0 34  0]\n",
      " [ 0  0  0  1  0  0  0  2  4 36]]\n",
      "\n",
      "[[31  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 37  0  0  0  0  0  0  1  0]\n",
      " [ 0  1 37  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 25  0  1  0  0  1  0]\n",
      " [ 0  0  0  0 41  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 34  0  0  0  1]\n",
      " [ 0  1  0  0  0  0 37  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 34  0  0]\n",
      " [ 0  0  0  0  1  0  0  0 34  0]\n",
      " [ 0  0  0  1  0  0  0  0  1 41]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' \\n손글씨 분류 모델로는 3번 SVM이 제일 유용할 것 같습니다. \\n왜냐하면, f1 score 값이 고르게 높고 100% 정확히 예측한 손글씨도 가장 많기 때문입니다.\\n즉, 예측한 손글씨 숫자 중 실제 숫자로 맞는 경우가 더 높게 나타나기 때문입니다.(정밀도)\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "\n",
    "# 필요한 모듈 import\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# 데이터 준비\n",
    "digits = load_digits()\n",
    "\n",
    "# 데이터 이해하기\n",
    "digits_data = digits.data            # Feature Data 지정하기\n",
    "digits_label = digits.target         # Label Data 지정하기\n",
    "print(digits.target_names)           # Target Names 출력해 보기\n",
    "print(digits_data[:20])             # 데이터 Describe 해 보기\n",
    "\n",
    "\n",
    "# train, test 데이터 분리\n",
    "X_train, X_test, y_train, y_test = train_test_split(digits_data,\n",
    "                                                    digits_label,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=15)\n",
    "\n",
    "# 다양한 모델로 학습하기\n",
    "\n",
    "# 1. Decision Tree 사용해 보기\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "decision_tree = DecisionTreeClassifier(random_state=15)\n",
    "decision_tree.fit(X_train, y_train)\n",
    "y_pred1 = decision_tree.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred1))\n",
    "\n",
    "# 2. Random Forest 사용해 보기\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "random_forest = RandomForestClassifier(random_state=32)\n",
    "random_forest.fit(X_train, y_train)\n",
    "y_pred2 = random_forest.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred2))\n",
    "\n",
    "# 3. SVM 사용해 보기\n",
    "from sklearn import svm\n",
    "\n",
    "svm_model = svm.SVC()\n",
    "svm_model.fit(X_train, y_train)\n",
    "y_pred3 = svm_model.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred3))\n",
    "\n",
    "# 4. SGD Classifier 사용해 보기\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd_model = SGDClassifier()\n",
    "sgd_model.fit(X_train, y_train)\n",
    "y_pred4 = sgd_model.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred4))\n",
    "\n",
    "# 5. Logistic Regression 사용해 보기\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logistic_model = LogisticRegression(random_state=32)\n",
    "logistic_model.fit(X_train, y_train)\n",
    "y_pred5 = logistic_model.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred5))\n",
    "\n",
    "\n",
    "# 모델을 평가해 보기\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred1), end='\\n\\n')\n",
    "print(confusion_matrix(y_test, y_pred2), end='\\n\\n')\n",
    "print(confusion_matrix(y_test, y_pred3), end='\\n\\n')\n",
    "print(confusion_matrix(y_test, y_pred4), end='\\n\\n')\n",
    "print(confusion_matrix(y_test, y_pred5), end='\\n\\n')\n",
    "\n",
    "''' \n",
    "손글씨 분류 모델로는 3번 SVM이 제일 유용할 것 같습니다. \n",
    "왜냐하면, f1 score 값이 고르게 높고 100% 정확히 예측한 손글씨도 가장 많기 때문입니다.\n",
    "즉, 예측한 손글씨 숫자 중 실제 숫자로 맞는 경우가 더 높게 나타나기 때문입니다.(정밀도)\n",
    "''' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b49e6966",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_result = classification_report(y_test, y_pred3)\n",
    "type(svm_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42da986a",
   "metadata": {},
   "source": [
    "#   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8f6b05",
   "metadata": {},
   "source": [
    "# 1-12. 프로젝트 (2) load_wine : 와인을 분류해 봅시다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7a5b94dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['class_0' 'class_1' 'class_2']\n",
      "[[1.423e+01 1.710e+00 2.430e+00 1.560e+01 1.270e+02 2.800e+00 3.060e+00\n",
      "  2.800e-01 2.290e+00 5.640e+00 1.040e+00 3.920e+00 1.065e+03]\n",
      " [1.320e+01 1.780e+00 2.140e+00 1.120e+01 1.000e+02 2.650e+00 2.760e+00\n",
      "  2.600e-01 1.280e+00 4.380e+00 1.050e+00 3.400e+00 1.050e+03]\n",
      " [1.316e+01 2.360e+00 2.670e+00 1.860e+01 1.010e+02 2.800e+00 3.240e+00\n",
      "  3.000e-01 2.810e+00 5.680e+00 1.030e+00 3.170e+00 1.185e+03]\n",
      " [1.437e+01 1.950e+00 2.500e+00 1.680e+01 1.130e+02 3.850e+00 3.490e+00\n",
      "  2.400e-01 2.180e+00 7.800e+00 8.600e-01 3.450e+00 1.480e+03]\n",
      " [1.324e+01 2.590e+00 2.870e+00 2.100e+01 1.180e+02 2.800e+00 2.690e+00\n",
      "  3.900e-01 1.820e+00 4.320e+00 1.040e+00 2.930e+00 7.350e+02]\n",
      " [1.420e+01 1.760e+00 2.450e+00 1.520e+01 1.120e+02 3.270e+00 3.390e+00\n",
      "  3.400e-01 1.970e+00 6.750e+00 1.050e+00 2.850e+00 1.450e+03]\n",
      " [1.439e+01 1.870e+00 2.450e+00 1.460e+01 9.600e+01 2.500e+00 2.520e+00\n",
      "  3.000e-01 1.980e+00 5.250e+00 1.020e+00 3.580e+00 1.290e+03]\n",
      " [1.406e+01 2.150e+00 2.610e+00 1.760e+01 1.210e+02 2.600e+00 2.510e+00\n",
      "  3.100e-01 1.250e+00 5.050e+00 1.060e+00 3.580e+00 1.295e+03]\n",
      " [1.483e+01 1.640e+00 2.170e+00 1.400e+01 9.700e+01 2.800e+00 2.980e+00\n",
      "  2.900e-01 1.980e+00 5.200e+00 1.080e+00 2.850e+00 1.045e+03]\n",
      " [1.386e+01 1.350e+00 2.270e+00 1.600e+01 9.800e+01 2.980e+00 3.150e+00\n",
      "  2.200e-01 1.850e+00 7.220e+00 1.010e+00 3.550e+00 1.045e+03]]\n",
      "<class 'numpy.ndarray'>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      1.00      0.96        12\n",
      "           1       0.91      0.83      0.87        12\n",
      "           2       0.92      0.92      0.92        12\n",
      "\n",
      "    accuracy                           0.92        36\n",
      "   macro avg       0.92      0.92      0.92        36\n",
      "weighted avg       0.92      0.92      0.92        36\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        12\n",
      "           1       1.00      1.00      1.00        12\n",
      "           2       1.00      1.00      1.00        12\n",
      "\n",
      "    accuracy                           1.00        36\n",
      "   macro avg       1.00      1.00      1.00        36\n",
      "weighted avg       1.00      1.00      1.00        36\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.92      0.85        12\n",
      "           1       0.50      0.92      0.65        12\n",
      "           2       0.00      0.00      0.00        12\n",
      "\n",
      "    accuracy                           0.61        36\n",
      "   macro avg       0.43      0.61      0.50        36\n",
      "weighted avg       0.43      0.61      0.50        36\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.92      0.81        12\n",
      "           1       0.53      0.83      0.65        12\n",
      "           2       1.00      0.17      0.29        12\n",
      "\n",
      "    accuracy                           0.64        36\n",
      "   macro avg       0.75      0.64      0.58        36\n",
      "weighted avg       0.75      0.64      0.58        36\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.92      0.96        12\n",
      "           1       0.86      1.00      0.92        12\n",
      "           2       1.00      0.92      0.96        12\n",
      "\n",
      "    accuracy                           0.94        36\n",
      "   macro avg       0.95      0.94      0.95        36\n",
      "weighted avg       0.95      0.94      0.95        36\n",
      "\n",
      "[[12  0  0]\n",
      " [ 1 10  1]\n",
      " [ 0  1 11]]\n",
      "\n",
      "[[12  0  0]\n",
      " [ 0 12  0]\n",
      " [ 0  0 12]]\n",
      "\n",
      "[[11  1  0]\n",
      " [ 1 11  0]\n",
      " [ 2 10  0]]\n",
      "\n",
      "[[11  1  0]\n",
      " [ 2 10  0]\n",
      " [ 2  8  2]]\n",
      "\n",
      "[[11  1  0]\n",
      " [ 0 12  0]\n",
      " [ 0  1 11]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' \\n와인 분류 모델로는 2번 Random forest가 제일 유용할 것 같습니다. \\n왜냐하면, 모든 부분(정밀도, 리콜, f1)에서 1을 기록했기 때문입니다.\\n'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (1) 필요한 모듈 import하기\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# (2) 데이터 준비\n",
    "wines = load_wine()\n",
    "\n",
    "# (3) 데이터 이해하기\n",
    "wines_data = wines.data              # Feature Data 지정하기\n",
    "wines_label = wines.target          # Label Data 지정하기\n",
    "print(wines.target_names)           # Target Names 출력해 보기\n",
    "print(wines_data[:10])             # 데이터 Describe 해 보기\n",
    "print(type(wines_data))\n",
    "\n",
    "# (4) train, test 데이터 분리\n",
    "X_train, X_test, y_train, y_test = train_test_split(wines_data,\n",
    "                                                    wines_label,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=15)\n",
    "\n",
    "# (5) 다양한 모델로 학습시켜보기\n",
    "\n",
    "# 1. Decision Tree 사용해 보기\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "decision_tree = DecisionTreeClassifier(random_state=15)\n",
    "decision_tree.fit(X_train, y_train)\n",
    "y_pred1 = decision_tree.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred1))\n",
    "\n",
    "# 2. Random Forest 사용해 보기\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "random_forest = RandomForestClassifier(random_state=32)\n",
    "random_forest.fit(X_train, y_train)\n",
    "y_pred2 = random_forest.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred2))\n",
    "\n",
    "# 3. SVM 사용해 보기\n",
    "from sklearn import svm\n",
    "\n",
    "svm_model = svm.SVC()\n",
    "svm_model.fit(X_train, y_train)\n",
    "y_pred3 = svm_model.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred3))\n",
    "\n",
    "# 4. SGD Classifier 사용해 보기\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd_model = SGDClassifier()\n",
    "sgd_model.fit(X_train, y_train)\n",
    "y_pred4 = sgd_model.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred4))\n",
    "\n",
    "# 5. Logistic Regression 사용해 보기\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logistic_model = LogisticRegression(random_state=32)\n",
    "logistic_model.fit(X_train, y_train)\n",
    "y_pred5 = logistic_model.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred5))\n",
    "\n",
    "\n",
    "# (6) 모델을 평가해 보기\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred1), end='\\n\\n')\n",
    "print(confusion_matrix(y_test, y_pred2), end='\\n\\n')\n",
    "print(confusion_matrix(y_test, y_pred3), end='\\n\\n')\n",
    "print(confusion_matrix(y_test, y_pred4), end='\\n\\n')\n",
    "print(confusion_matrix(y_test, y_pred5), end='\\n\\n')\n",
    "\n",
    "''' \n",
    "와인 분류 모델로는 2번 Random forest가 제일 유용할 것 같습니다. \n",
    "왜냐하면, 모든 부분(정밀도, 리콜, f1)에서 1을 기록했기 때문입니다.\n",
    "''' "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24956ff6",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4679fda",
   "metadata": {},
   "source": [
    "# 1-13. 프로젝트 (3) load_breast_cancer : 유방암 여부를 진단해 봅시다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "de4a3282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['malignant' 'benign']\n",
      "[[1.799e+01 1.038e+01 1.228e+02 1.001e+03 1.184e-01 2.776e-01 3.001e-01\n",
      "  1.471e-01 2.419e-01 7.871e-02 1.095e+00 9.053e-01 8.589e+00 1.534e+02\n",
      "  6.399e-03 4.904e-02 5.373e-02 1.587e-02 3.003e-02 6.193e-03 2.538e+01\n",
      "  1.733e+01 1.846e+02 2.019e+03 1.622e-01 6.656e-01 7.119e-01 2.654e-01\n",
      "  4.601e-01 1.189e-01]\n",
      " [2.057e+01 1.777e+01 1.329e+02 1.326e+03 8.474e-02 7.864e-02 8.690e-02\n",
      "  7.017e-02 1.812e-01 5.667e-02 5.435e-01 7.339e-01 3.398e+00 7.408e+01\n",
      "  5.225e-03 1.308e-02 1.860e-02 1.340e-02 1.389e-02 3.532e-03 2.499e+01\n",
      "  2.341e+01 1.588e+02 1.956e+03 1.238e-01 1.866e-01 2.416e-01 1.860e-01\n",
      "  2.750e-01 8.902e-02]\n",
      " [1.969e+01 2.125e+01 1.300e+02 1.203e+03 1.096e-01 1.599e-01 1.974e-01\n",
      "  1.279e-01 2.069e-01 5.999e-02 7.456e-01 7.869e-01 4.585e+00 9.403e+01\n",
      "  6.150e-03 4.006e-02 3.832e-02 2.058e-02 2.250e-02 4.571e-03 2.357e+01\n",
      "  2.553e+01 1.525e+02 1.709e+03 1.444e-01 4.245e-01 4.504e-01 2.430e-01\n",
      "  3.613e-01 8.758e-02]\n",
      " [1.142e+01 2.038e+01 7.758e+01 3.861e+02 1.425e-01 2.839e-01 2.414e-01\n",
      "  1.052e-01 2.597e-01 9.744e-02 4.956e-01 1.156e+00 3.445e+00 2.723e+01\n",
      "  9.110e-03 7.458e-02 5.661e-02 1.867e-02 5.963e-02 9.208e-03 1.491e+01\n",
      "  2.650e+01 9.887e+01 5.677e+02 2.098e-01 8.663e-01 6.869e-01 2.575e-01\n",
      "  6.638e-01 1.730e-01]\n",
      " [2.029e+01 1.434e+01 1.351e+02 1.297e+03 1.003e-01 1.328e-01 1.980e-01\n",
      "  1.043e-01 1.809e-01 5.883e-02 7.572e-01 7.813e-01 5.438e+00 9.444e+01\n",
      "  1.149e-02 2.461e-02 5.688e-02 1.885e-02 1.756e-02 5.115e-03 2.254e+01\n",
      "  1.667e+01 1.522e+02 1.575e+03 1.374e-01 2.050e-01 4.000e-01 1.625e-01\n",
      "  2.364e-01 7.678e-02]\n",
      " [1.245e+01 1.570e+01 8.257e+01 4.771e+02 1.278e-01 1.700e-01 1.578e-01\n",
      "  8.089e-02 2.087e-01 7.613e-02 3.345e-01 8.902e-01 2.217e+00 2.719e+01\n",
      "  7.510e-03 3.345e-02 3.672e-02 1.137e-02 2.165e-02 5.082e-03 1.547e+01\n",
      "  2.375e+01 1.034e+02 7.416e+02 1.791e-01 5.249e-01 5.355e-01 1.741e-01\n",
      "  3.985e-01 1.244e-01]\n",
      " [1.825e+01 1.998e+01 1.196e+02 1.040e+03 9.463e-02 1.090e-01 1.127e-01\n",
      "  7.400e-02 1.794e-01 5.742e-02 4.467e-01 7.732e-01 3.180e+00 5.391e+01\n",
      "  4.314e-03 1.382e-02 2.254e-02 1.039e-02 1.369e-02 2.179e-03 2.288e+01\n",
      "  2.766e+01 1.532e+02 1.606e+03 1.442e-01 2.576e-01 3.784e-01 1.932e-01\n",
      "  3.063e-01 8.368e-02]\n",
      " [1.371e+01 2.083e+01 9.020e+01 5.779e+02 1.189e-01 1.645e-01 9.366e-02\n",
      "  5.985e-02 2.196e-01 7.451e-02 5.835e-01 1.377e+00 3.856e+00 5.096e+01\n",
      "  8.805e-03 3.029e-02 2.488e-02 1.448e-02 1.486e-02 5.412e-03 1.706e+01\n",
      "  2.814e+01 1.106e+02 8.970e+02 1.654e-01 3.682e-01 2.678e-01 1.556e-01\n",
      "  3.196e-01 1.151e-01]\n",
      " [1.300e+01 2.182e+01 8.750e+01 5.198e+02 1.273e-01 1.932e-01 1.859e-01\n",
      "  9.353e-02 2.350e-01 7.389e-02 3.063e-01 1.002e+00 2.406e+00 2.432e+01\n",
      "  5.731e-03 3.502e-02 3.553e-02 1.226e-02 2.143e-02 3.749e-03 1.549e+01\n",
      "  3.073e+01 1.062e+02 7.393e+02 1.703e-01 5.401e-01 5.390e-01 2.060e-01\n",
      "  4.378e-01 1.072e-01]\n",
      " [1.246e+01 2.404e+01 8.397e+01 4.759e+02 1.186e-01 2.396e-01 2.273e-01\n",
      "  8.543e-02 2.030e-01 8.243e-02 2.976e-01 1.599e+00 2.039e+00 2.394e+01\n",
      "  7.149e-03 7.217e-02 7.743e-02 1.432e-02 1.789e-02 1.008e-02 1.509e+01\n",
      "  4.068e+01 9.765e+01 7.114e+02 1.853e-01 1.058e+00 1.105e+00 2.210e-01\n",
      "  4.366e-01 2.075e-01]]\n",
      "<class 'numpy.ndarray'>\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.87      0.92        39\n",
      "           1       0.94      0.99      0.96        75\n",
      "\n",
      "    accuracy                           0.95       114\n",
      "   macro avg       0.95      0.93      0.94       114\n",
      "weighted avg       0.95      0.95      0.95       114\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.87      0.91        39\n",
      "           1       0.94      0.97      0.95        75\n",
      "\n",
      "    accuracy                           0.94       114\n",
      "   macro avg       0.94      0.92      0.93       114\n",
      "weighted avg       0.94      0.94      0.94       114\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.67      0.78        39\n",
      "           1       0.85      0.97      0.91        75\n",
      "\n",
      "    accuracy                           0.87       114\n",
      "   macro avg       0.89      0.82      0.84       114\n",
      "weighted avg       0.88      0.87      0.86       114\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.87      0.76        39\n",
      "           1       0.92      0.77      0.84        75\n",
      "\n",
      "    accuracy                           0.81       114\n",
      "   macro avg       0.79      0.82      0.80       114\n",
      "weighted avg       0.83      0.81      0.81       114\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.85      0.85        39\n",
      "           1       0.92      0.92      0.92        75\n",
      "\n",
      "    accuracy                           0.89       114\n",
      "   macro avg       0.88      0.88      0.88       114\n",
      "weighted avg       0.89      0.89      0.89       114\n",
      "\n",
      "[[34  5]\n",
      " [ 1 74]]\n",
      "\n",
      "[[34  5]\n",
      " [ 2 73]]\n",
      "\n",
      "[[26 13]\n",
      " [ 2 73]]\n",
      "\n",
      "[[34  5]\n",
      " [17 58]]\n",
      "\n",
      "[[33  6]\n",
      " [ 6 69]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' \\n와인 분류 모델로는 2번 Random forest가 제일 유용할 것 같습니다. \\n왜냐하면, 모든 부분(정밀도, 리콜, f1)에서 1을 기록했기 때문입니다.\\n'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (1) 필요한 모듈 import하기\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# (2) 데이터 준비\n",
    "bc = load_breast_cancer()\n",
    "\n",
    "# (3) 데이터 이해하기\n",
    "bc_data = bc.data              # Feature Data 지정하기\n",
    "bc_label = bc.target          # Label Data 지정하기\n",
    "print(bc.target_names)           # Target Names 출력해 보기\n",
    "print(bc_data[:10])             # 데이터 Describe 해 보기\n",
    "print(type(bc_data))\n",
    "\n",
    "# (4) train, test 데이터 분리\n",
    "X_train, X_test, y_train, y_test = train_test_split(bc_data,\n",
    "                                                    bc_label,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=15)\n",
    "\n",
    "# (5) 다양한 모델로 학습시켜보기\n",
    "\n",
    "# 1. Decision Tree 사용해 보기\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "decision_tree = DecisionTreeClassifier(random_state=15)\n",
    "decision_tree.fit(X_train, y_train)\n",
    "y_pred1 = decision_tree.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred1))\n",
    "\n",
    "# 2. Random Forest 사용해 보기\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "random_forest = RandomForestClassifier(random_state=32)\n",
    "random_forest.fit(X_train, y_train)\n",
    "y_pred2 = random_forest.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred2))\n",
    "\n",
    "# 3. SVM 사용해 보기\n",
    "from sklearn import svm\n",
    "\n",
    "svm_model = svm.SVC()\n",
    "svm_model.fit(X_train, y_train)\n",
    "y_pred3 = svm_model.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred3))\n",
    "\n",
    "# 4. SGD Classifier 사용해 보기\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd_model = SGDClassifier()\n",
    "sgd_model.fit(X_train, y_train)\n",
    "y_pred4 = sgd_model.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred4))\n",
    "\n",
    "# 5. Logistic Regression 사용해 보기\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logistic_model = LogisticRegression(random_state=32)\n",
    "logistic_model.fit(X_train, y_train)\n",
    "y_pred5 = logistic_model.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred5))\n",
    "\n",
    "\n",
    "# (6) 모델을 평가해 보기\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred1), end='\\n\\n')\n",
    "print(confusion_matrix(y_test, y_pred2), end='\\n\\n')\n",
    "print(confusion_matrix(y_test, y_pred3), end='\\n\\n')\n",
    "print(confusion_matrix(y_test, y_pred4), end='\\n\\n')\n",
    "print(confusion_matrix(y_test, y_pred5), end='\\n\\n')\n",
    "\n",
    "''' \n",
    "유방암 분류 모델로는 1번 Decision Tree가 제일 유용할 것 같습니다. \n",
    "왜냐하면, 유방암의 경우 리콜값 높게 나오는 게 중요하기 때문입니다.\n",
    "즉, 실제 악성인 경우 양성으로 판단하는 오류가 치명적일 수 있습니다.\n",
    "또는 양성인 경우 정확히 양성을 판단해야 합니다. \n",
    "그런 점에서 1번 모델이 가장 유용한 결과를 내고 있습니다.\n",
    "''' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d439eed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
